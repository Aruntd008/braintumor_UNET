{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Channel Expansion and Reduction:\n",
    "The expansion_ratio is used to temporarily increase the number of channels within a block before reducing it back to the desired number of output channels. This process involves:\n",
    "\n",
    "Expanding the number of channels: By increasing the number of channels by a certain ratio (e.g., 4x), the model can learn richer and more complex features.\n",
    "Reducing back to the original dimension: After expanding and applying non-linear transformations, the number of channels is reduced back to the desired output dimension, which helps in maintaining computational efficiency.\n",
    "2. Increased Capacity without High Computational Cost:\n",
    "By using the expansion_ratio, the model can afford to learn more detailed features without significantly increasing the computational cost. This is because the convolutions with expanded channels are often followed by pointwise (1x1) convolutions, which are computationally cheaper than spatial convolutions with larger kernels.\n",
    "\n",
    "3. Efficiency in Deep Networks:\n",
    "Deep networks benefit from the bottleneck design as it:\n",
    "\n",
    "Reduces the number of parameters: Pointwise convolutions (1x1) are efficient in terms of the number of parameters and computations.\n",
    "Improves training dynamics: Bottleneck structures help in efficient gradient flow, making the training of very deep networks more feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51475, 5720)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "\n",
    "# Directory containing .h5 files\n",
    "directory = \"./braTS/BraTS2020_training_data/content/data\"\n",
    "\n",
    "# Create a list of all .h5 files in the directory\n",
    "h5_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.h5')]\n",
    "\n",
    "train_size = int(0.9 * len(h5_files))\n",
    "test_size = len(h5_files) - train_size\n",
    "train_size, test_size \n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.facecolor'] = '#171717'\n",
    "plt.rcParams['text.color']       = '#DDDDDD'\n",
    "\n",
    "def display_images(image, title='Image Channels', save_path=None):\n",
    "    channel_names = ['T1-weighted (T1)', 'T1-weighted post contrast (T1c)', 'T2-weighted (T2)', 'Fluid Attenuated Inversion Recovery (FLAIR)']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        channel_image = image[idx, :, :]  # Transpose the array to display the channel\n",
    "        ax.imshow(channel_image, cmap='magma')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(channel_names[idx])\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=20, y=1.03)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "\n",
    "def display_masks(mask, title='Mask Channels', save_path=None):\n",
    "    channel_names = ['Necrotic (NEC)', 'Edema (ED)', 'Tumour (ET)']\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(9.75, 5))\n",
    "    for idx, ax in enumerate(axes):\n",
    "        rgb_mask = np.zeros((mask.shape[1], mask.shape[2], 3), dtype=np.uint8)\n",
    "        rgb_mask[..., idx] = mask[idx, :, :] * 255  # Transpose the array to display the channel\n",
    "        ax.imshow(rgb_mask)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(channel_names[idx])\n",
    "    plt.suptitle(title, fontsize=20, y=0.93)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_test_sample(model, test_input, test_target, device, save_path=None):\n",
    "    test_input, test_target = test_input.to(device), test_target.to(device)\n",
    "\n",
    "    # Obtain the model's prediction\n",
    "    test_pred = torch.sigmoid(model(test_input))\n",
    "\n",
    "    # Process the image and masks for visualization\n",
    "    image = test_input.detach().cpu().numpy().squeeze(0)\n",
    "    mask_pred = test_pred.detach().cpu().numpy().squeeze(0)\n",
    "    mask_target = test_target.detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "    # Set the plot aesthetics\n",
    "    plt.rcParams['figure.facecolor'] = '#171717'\n",
    "    plt.rcParams['text.color']       = '#DDDDDD'\n",
    "\n",
    "    # Display the input image, predicted mask, and target mask\n",
    "    fig_image = display_images(image)\n",
    "    fig_pred_mask = display_masks(mask_pred, title='Predicted Mask Channels')\n",
    "    fig_target_mask = display_masks(mask_target, title='Ground Truth')\n",
    "\n",
    "    if save_path:\n",
    "        fig_image.savefig(f\"{save_path}_image.png\")\n",
    "        fig_pred_mask.savefig(f\"{save_path}_pred_mask.png\")\n",
    "        fig_target_mask.savefig(f\"{save_path}_target_mask.png\")\n",
    "        plt.close(fig_image)\n",
    "        plt.close(fig_pred_mask)\n",
    "        plt.close(fig_target_mask)\n",
    "    else:\n",
    "        return fig_image, fig_pred_mask, fig_target_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 128\n",
    "N_CHANNELS = 4\n",
    "N_CLASSES = 4\n",
    "\n",
    "class ENResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, channel_ratio=4):\n",
    "        super(ENResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=1, padding=3, groups=in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, channel_ratio*out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_ratio*out_channels, out_channels, kernel_size=1, stride=1)\n",
    "       \n",
    "       \n",
    "        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=3, groups=out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv5 = nn.Conv2d(out_channels, channel_ratio*out_channels, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(channel_ratio*out_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1 , stride=1)\n",
    "        self.residual_bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual_conv(x)\n",
    "        residual = self.residual_bn(residual)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.conv5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv6(out)\n",
    "        \n",
    "        out += residual\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class DEResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7,channel_ratio=4):\n",
    "        super(DEResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=1, padding=3, groups=in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, channel_ratio*in_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_ratio*in_channels, out_channels, kernel_size=1, stride=1)\n",
    "       \n",
    "       \n",
    "        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=3, groups=out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv5 = nn.Conv2d(out_channels, channel_ratio*out_channels, kernel_size=1, stride=1 )\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(channel_ratio*out_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1 , stride=1)\n",
    "        self.residual_bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual_conv(x)\n",
    "        residual = self.residual_bn(residual)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.conv5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv6(out)\n",
    "        \n",
    "        out += residual\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class BottleneckResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, channel_ratio=4):\n",
    "        super().__init__()\n",
    "        mid_channels = channel_ratio * in_channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=7, stride=1, padding=3, groups=in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(mid_channels, in_channels, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels, in_channels, kernel_size=7, stride=1, padding=3, groups=in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv5 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(mid_channels, in_channels, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.conv5(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv6(out)\n",
    "        \n",
    "        \n",
    "        out += identity\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, g_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(g_channels, in_channels, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=2),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=1,  stride=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        \n",
    "        psi = self.psi(torch.relu(g1 + x1))\n",
    "        ogX_size = self.upsample(psi)\n",
    "        return x * ogX_size\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels = 4 , out_channels = 3 ):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Up and downsample layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        \n",
    "        self.encoder1 = ENResidualBlock(in_channels, 64)\n",
    "        self.encoder2 = ENResidualBlock(64, 128)\n",
    "        self.encoder3 = ENResidualBlock(128, 256)\n",
    "        self.encoder4 = ENResidualBlock(256, 512)\n",
    "\n",
    "        self.bottleneck = BottleneckResidualBlock(512)\n",
    "\n",
    "        self.att4 = AttentionBlock(512, 512)\n",
    "        self.decoder4 = DEResidualBlock(512, 256)\n",
    "        self.att3 = AttentionBlock(256, 256)\n",
    "        self.decoder3 = DEResidualBlock(256, 128)\n",
    "        self.att2 = AttentionBlock(128, 128)\n",
    "        self.decoder2 = DEResidualBlock(128, 64)\n",
    "        self.att1 = AttentionBlock(64, 64)\n",
    "        self.decoder1 = DEResidualBlock(64, 3)\n",
    "        \n",
    "        self.output_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))  \n",
    "        e3 = self.encoder3(self.pool(e2))  \n",
    "        e4 = self.encoder4(self.pool(e3))  \n",
    "\n",
    "        b = self.bottleneck(self.pool(e4)) \n",
    "       \n",
    "        d4 = self.decoder4(self.att4(b, e4) + self.upsample(b))\n",
    "        d3 = self.decoder3(self.att3(d4, e3) + self.upsample(d4))\n",
    "        d2 = self.decoder2(self.att2(d3, e2) + self.upsample(d3))\n",
    "        d1 = self.decoder1(self.att1(d2, e1) + self.upsample(d2))\n",
    "        \n",
    "       \n",
    "        return d1\n",
    "    \n",
    "    \n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'Total Parameters: {total_params:,}\\n')\n",
    "\n",
    "def save_model(model, path='model_weights.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "# Dataset class\n",
    "class BRATSDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            image = file['image'][()]\n",
    "            mask = file['mask'][()]\n",
    "            \n",
    "            # bring the channels to the index 0\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            mask = np.transpose(mask, (2, 0, 1))\n",
    "            \n",
    "            # pixel values for each channel to 0 and 255\n",
    "            min_val = image.min(axis=(1, 2), keepdims=True)\n",
    "            image -= min_val  \n",
    "\n",
    "            ptp_val = image.ptp(axis=(1, 2), keepdims=True) + 1e-4\n",
    "            image /= ptp_val  # Scale max to 1\n",
    "\n",
    "            image = torch.tensor(image, dtype=torch.float32)\n",
    "            mask = torch.tensor(mask, dtype=torch.float32)\n",
    "            \n",
    "            # if self.transform:\n",
    "            #     image, mask = self.transform(image, mask)\n",
    "            \n",
    "        return image, mask\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5] * N_CHANNELS, std=[0.5] * N_CHANNELS)\n",
    "])\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BRATSDataset(h5_files, transform=transform)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "\n",
    "test_input_iterator = iter(DataLoader(test_dataset, batch_size=1, shuffle=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10295"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  49%|████▊     | 5013/10295 [23:05<24:20,  3.62batch/s, Epoch loss=0.171993, Batch loss=0.010498]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Use tqdm to create a progress bar for the training loop\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_dataloader, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tepoch:\n\u001b[1;32m---> 33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtepoch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_description\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[4], line 227\u001b[0m, in \u001b[0;36mBRATSDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    225\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_paths[idx]\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 227\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    228\u001b[0m     mask \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m][()]\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# bring the channels to the index 0\u001b[39;00m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\h5py\\_hl\\dataset.py:758\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, args, new_dtype)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 758\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    760\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Define BCE Loss with Logits\n",
    "class BCELossWithLogits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCELossWithLogits, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.bce_loss(inputs, targets)\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = UNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = BCELossWithLogits()\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50  # Number of epochs to train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for i, (images, masks) in enumerate(tepoch, 1):\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "            tepoch.set_postfix({\"Epoch loss\": f'{(running_loss/i):.6f}',\n",
    "                                \"Batch loss\": f'{loss.item():.6f}'})\n",
    "            \n",
    "            if i % 100 == 0:  # Save images every 10 batches\n",
    "                save_path = f\"images/epoch_{epoch+1}_batch_{i}\"\n",
    "                display_test_sample(model, images[0:1], masks[0:1], device, save_path=save_path)\n",
    "            \n",
    "            # Delete variables to free up memory\n",
    "            # del images, masks, outputs, loss\n",
    "    \n",
    "    # Save model at the end of each epoch\n",
    "    save_model(model, f'models/model_epoch_{epoch+1}.pth')\n",
    "\n",
    "# Evaluation Loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(test_dataloader, unit=\"batch\") as ttest:\n",
    "        for j, (images, masks) in enumerate(ttest, 1):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "        \n",
    "            test_loss += loss.item()\n",
    "            ttest.set_postfix({\"Epoch loss\": f'{(test_loss/j):.6f}',\n",
    "                               \"Batch loss\": f'{loss.item():.6f}'})\n",
    "            \n",
    "            if j % 100 == 0:  # Save images every 10 batches\n",
    "                save_path = f\"test/test_batch_{j}\"\n",
    "                display_test_sample(model, images[0:1], masks[0:1], device, save_path=save_path)\n",
    "            \n",
    "            # Delete variables to free up memory\n",
    "            del images, masks, outputs, loss\n",
    "\n",
    "# Save the final model\n",
    "save_model(model, 'final_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
